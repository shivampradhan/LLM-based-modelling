{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from secret_key import openapi_key\n",
    "# import os\n",
    "# os.environ['OPENAI_API_KEY'] = openapi_key\n",
    "\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"add your key here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# open ai based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.9)\n",
    "name = llm.predict(\"I want to open a restaurant for Indian food. Suggest a fency name for this.\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mllm\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI want to open a restaurant for Indian food. Suggest a fency name for this.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSaffron & Spice.\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "llm(\"I want to open a restaurant for Indian food. Suggest a fency name for this.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "p = prompt_template_name.format(cuisine=\"Italian\")\n",
    "print(p)\n",
    "\n",
    "# print(llm.predict(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "chain.run(\"Mexican\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt_template_name, verbose=True)\n",
    "chain.run(\"Mexican\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.6)\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n",
    "name_chain =LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template=\"\"\"Suggest some menu items for {restaurant_name}\"\"\"\n",
    ")\n",
    "\n",
    "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3111096976.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    Simple Sequential Chain\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Simple Sequential Chain\n",
    "\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "chain = SimpleSequentialChain(chains = [name_chain, food_items_chain])\n",
    "\n",
    "content = chain.run(\"Indian\")\n",
    "print(content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sequential Chain\n",
    "\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n",
    "name_chain =LLMChain(llm=llm, prompt=prompt_template_name, output_key=\"restaurant_name\")\n",
    "\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template=\"Suggest some menu items for {restaurant_name}.\"\n",
    ")\n",
    "\n",
    "food_items_chain =LLMChain(llm=llm, prompt=prompt_template_items, output_key=\"menu_items\")\n",
    "\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "chain = SequentialChain(\n",
    "    chains = [name_chain, food_items_chain],\n",
    "    input_variables = ['cuisine'],\n",
    "    output_variables = ['restaurant_name', \"menu_items\"]\n",
    ")\n",
    "\n",
    "chain({\"cuisine\": \"Indian\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Agents\n",
    "\n",
    "# make sure yoy have installed this package: pip install google-search-results\n",
    "# from secret_key import serpapi_key\n",
    "# os.environ['SERPAPI_API_KEY'] = serpapi_key\n",
    "\n",
    "os.environ['SERPAPI_API_KEY'] = \"add your serpapi key here\"\n",
    "\n",
    "#serpapi and llm-math tool\n",
    "\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Let's test it out!\n",
    "agent.run(\"What was the GDP of US in 2022 plus 5?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Wikipedia and llm-math tool\n",
    "\n",
    "# install this package: pip install wikipedia\n",
    "\n",
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Let's test it out!\n",
    "agent.run(\"When was Elon musk born? What is his age right now in 2023?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4175289062.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    Taco Fiesta\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm,prompt=prompt_template_name)\n",
    "name = chain.run(\"Mexican\")\n",
    "print(name)\n",
    "\n",
    "\n",
    "\n",
    "name = chain.run(\"Indian\")\n",
    "print(name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "chain.memory\n",
    "\n",
    "type(chain.memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ConversationBufferMemory\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template_name, memory=memory)\n",
    "name = chain.run(\"Mexican\")\n",
    "print(name)\n",
    "\n",
    "name = chain.run(\"Arabic\")\n",
    "print(name)\n",
    "\n",
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ConversationChain\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "convo = ConversationChain(llm=OpenAI(temperature=0.7))\n",
    "print(convo.prompt.template)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(convo.run(\"Who won the first cricket world cup?\"))\n",
    "print(convo.run(\"How much is 5+5?\"))\n",
    "print(convo.run(\"Who was the captain ofthe winning team?\"))\n",
    "\n",
    "print(convo.memory.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ConversationBufferWindowMemory\n",
    "\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "\n",
    "convo = ConversationChain(\n",
    "    llm=OpenAI(temperature=0.7),\n",
    "    memory=memory\n",
    ")\n",
    "print(convo.run(\"Who won the first cricket world cup?\"))\n",
    "print(convo.run(\"How much is 5+5?\"))\n",
    "print(convo.run(\"Who was the captain of the winning team?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# faiss_tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Packages\n",
    "!pip install faiss-cpu\n",
    "!pip install sentence-transformers\n",
    "\n",
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "df = pd.read_csv(\"sample_text.csv\")\n",
    "df.shape\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1 : Create source embeddings for the text column\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "encoder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "vectors = encoder.encode(df.text)\n",
    "\n",
    "print(vectors.shape)\n",
    "\n",
    "# Step 2 : Build a FAISS Index for vectors\n",
    "\n",
    "import faiss\n",
    "\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "\n",
    "# Step 3 : Normalize the source vectors (as we are using L2 distance to measure similarity) and add to the index\n",
    "\n",
    "index.add(vectors)\n",
    "print(index)\n",
    "\n",
    "\n",
    "# Step 4 : Encode search text using same encorder and normalize the output vector\n",
    "\n",
    "search_query = \"I want to buy a polo t-shirt\"\n",
    "# search_query = \"looking for places to visit during the holidays\"\n",
    "# search_query = \"An apple a day keeps the doctor away\"\n",
    "vec = encoder.encode(search_query)\n",
    "vec.shape\n",
    "\n",
    "import numpy as np\n",
    "svec = np.array(vec).reshape(1,-1)\n",
    "svec.shape\n",
    "\n",
    "\n",
    "\n",
    "# faiss.normalize_L2(svec)\n",
    "\n",
    "dim = vectors.shape[1]\n",
    "print(dim)\n",
    "\n",
    "\n",
    "#Step 5: Search for similar vector in the FAISS index created\n",
    "\n",
    "distances, I = index.search(new_vec, k=2)\n",
    "row_indices = I.tolist()[0]\n",
    "print(distances,I.tolist(),row_indices)\n",
    "df.loc[row_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "search_query\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# blog usecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "import pickle\n",
    "import time\n",
    "import langchain\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "#load openAI api key\n",
    "#os.environ['OPENAI_API_KEY'] = 'your openapi key here'\n",
    "\n",
    "# Initialise LLM with required params\n",
    "llm = OpenAI(temperature=0.9, max_tokens=500) \n",
    "\n",
    "#(1) Load data\n",
    "\n",
    "loaders = UnstructuredURLLoader(urls=[\n",
    "    \"https://www.moneycontrol.com/news/business/markets/wall-street-rises-as-tesla-soars-on-ai-optimism-11351111.html\",\n",
    "    \"https://www.moneycontrol.com/news/business/tata-motors-launches-punch-icng-price-starts-at-rs-7-1-lakh-11098751.html\"\n",
    "])\n",
    "data = loaders.load() \n",
    "len(data)\n",
    "\n",
    "\n",
    "#(2) Split data to create chunks\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "# As data is of type documents we can directly use split_documents over split_text in order to get the chunks.\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "len(docs)\n",
    "\n",
    "\n",
    "\n",
    "docs[0]\n",
    "\n",
    "\n",
    "#(3) Create embeddings for these chunks and save them to FAISS index\n",
    "\n",
    "# Create the embeddings of the chunks using openAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Pass the documents and embeddings inorder to create FAISS vector index\n",
    "vectorindex_openai = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Storing vector index create in local\n",
    "file_path=\"vector_index.pkl\"\n",
    "with open(file_path, \"wb\") as f:\n",
    "    pickle.dump(vectorindex_openai, f)\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        vectorIndex = pickle.load(f)\n",
    "\n",
    "#(4) Retrieve similar embeddings for a given question and call LLM to retrieve final answer\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=vectorIndex.as_retriever())\n",
    "chain\n",
    "\n",
    "\n",
    "\n",
    "query = \"what is the price of Tiago iCNG?\"\n",
    "# query = \"what are the main features of punch iCNG?\"\n",
    "\n",
    "langchain.debug=True\n",
    "\n",
    "chain({\"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Loaders In LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TextLoader\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"nvda_news_1.txt\")\n",
    "loader.load()\n",
    "\n",
    "\n",
    "\n",
    "type(loader)\n",
    "loader.file_path\n",
    "\n",
    "\n",
    "\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path=\"movies.csv\")\n",
    "data = loader.load()\n",
    "data\n",
    "\n",
    "\n",
    "data[0]\n",
    "\n",
    "loader = CSVLoader(file_path=\"movies.csv\", source_column=\"title\")\n",
    "data = loader.load()\n",
    "data\n",
    "\n",
    "data[0].page_content\n",
    "data[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing necessary libraries, libmagic is used for file type detection\n",
    "!pip3 install unstructured libmagic python-magic python-magic-bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "\n",
    "loader = UnstructuredURLLoader(\n",
    "    urls = [\n",
    "        \"https://www.moneycontrol.com/news/business/banks/hdfc-bank-re-appoints-sanmoy-chakrabarti-as-chief-risk-officer-11259771.html\",\n",
    "        \"https://www.moneycontrol.com/news/business/markets/market-corrects-post-rbi-ups-inflation-forecast-icrr-bet-on-these-top-10-rate-sensitive-stocks-ideas-11142611.html\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "data = loader.load()\n",
    "len(data)\n",
    "\n",
    "data[0].page_content[0:100]\n",
    "data[0].metadata\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "text = \"\"\"Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan. \n",
    "It stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine. \n",
    "Set in a dystopian future where humanity is embroiled in a catastrophic blight and famine, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for humankind.\n",
    "\n",
    "Brothers Christopher and Jonathan Nolan wrote the screenplay, which had its origins in a script Jonathan developed in 2007 and was originally set to be directed by Steven Spielberg. \n",
    "Kip Thorne, a Caltech theoretical physicist and 2017 Nobel laureate in Physics,[4] was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar. \n",
    "Cinematographer Hoyte van Hoytema shot it on 35 mm movie film in the Panavision anamorphic format and IMAX 70 mm. Principal photography began in late 2013 and took place in Alberta, Iceland, and Los Angeles. \n",
    "Interstellar uses extensive practical and miniature effects, and the company Double Negative created additional digital effects.\n",
    "\n",
    "Interstellar premiered in Los Angeles on October 26, 2014. In the United States, it was first released on film stock, expanding to venues using digital projectors. The film received generally positive reviews from critics and grossed over $677 million worldwide ($715 million after subsequent re-releases), making it the tenth-highest-grossing film of 2014. \n",
    "It has been praised by astronomers for its scientific accuracy and portrayal of theoretical astrophysics.[5][6][7] Interstellar was nominated for five awards at the 87th Academy Awards, winning Best Visual Effects, and received numerous other accolades.\"\"\"\n",
    "\n",
    "#Manual approach of splitting the text into chunks\n",
    "\n",
    "# Say LLM token limit is 100, in that case we can do simple thing such as this\n",
    "# Well but we want complete words and want to do this for entire text, may be we can use Python's split funciton\n",
    "\n",
    "words = text.split(\" \")\n",
    "len(words)\n",
    "\n",
    "\n",
    "\n",
    "chunks = []\n",
    "\n",
    "s = \"\"\n",
    "for word in words:\n",
    "    s += word + \" \"\n",
    "    if len(s)>200:\n",
    "        chunks.append(s)\n",
    "        s = \"\"\n",
    "        \n",
    "chunks.append(s)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)\n",
    "len(chunks)\n",
    "\n",
    "\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(len(chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# google palm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Basic working of Google Palm LLM in LangChain\n",
    "\n",
    "from langchain.llms import GooglePalm\n",
    "\n",
    "api_key = 'your api key here' # get this free api key from https://makersuite.google.com/\n",
    "\n",
    "llm = GooglePalm(google_api_key=api_key, temperature=0.1)\n",
    "\n",
    "poem = llm(\"Write a 4 line poem of my love for samosa\")\n",
    "print(poem)\n",
    "\n",
    "essay = llm(\"write email requesting refund for electronic item\")\n",
    "print(essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "from langchain.llms import GooglePalm\n",
    "\n",
    "#Now let's load data from Codebasics FAQ csv file\n",
    "\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path='codebasics_faqs.csv', source_column=\"prompt\")\n",
    "\n",
    "# Store the loaded data in the 'data' variable\n",
    "data = loader.load()\n",
    "\n",
    "#Hugging Face Embeddings\n",
    "\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "# Initialize instructor embeddings using the Hugging Face model\n",
    "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-large\")\n",
    "\n",
    "e = instructor_embeddings.embed_query(\"What is your refund policy?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(e),e[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Vector store using FAISS\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Create a FAISS instance for vector database from 'data'\n",
    "vectordb = FAISS.from_documents(documents=data,\n",
    "                                 embedding=instructor_embeddings)\n",
    "\n",
    "# Create a retriever for querying the vector database\n",
    "retriever = vectordb.as_retriever(score_threshold = 0.7)\n",
    "\n",
    "rdocs = retriever.get_relevant_documents(\"how about job placement support?\")\n",
    "rdocs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Embeddings can be created using GooglePalm too. Also for vector database you can use chromadb as well as shown below. During our experimentation, we found hugging face embeddings and FAISS to be more appropriate for our use case\n",
    "\n",
    "# google_palm_embeddings = GooglePalmEmbeddings(google_api_key=api_key)\n",
    "\n",
    "# from langchain.vectorstores import Chroma\n",
    "# vectordb = Chroma.from_documents(data,\n",
    "#                            embedding=google_palm_embeddings,\n",
    "#                            persist_directory='./chromadb')\n",
    "# vectordb.persist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create RetrievalQA chain along with prompt template 🚀\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Given the following context and a question, generate an answer based on this context only.\n",
    "In the answer try to provide as much text as possible from \"response\" section in the source document context without making much changes.\n",
    "If the answer is not found in the context, kindly state \"I don't know.\" Don't try to make up an answer.\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "QUESTION: {question}\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                            chain_type=\"stuff\",\n",
    "                            retriever=retriever,\n",
    "                            input_key=\"query\",\n",
    "                            return_source_documents=True,\n",
    "                            chain_type_kwargs=chain_type_kwargs)\n",
    "\n",
    "\n",
    "chain('Do you provide job assistance and also do you provide job gurantee?')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain(\"Do you guys provide internship and also do you offer EMI payments?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Google Palm LLM & API key setup\n",
    "\n",
    "from langchain.llms import GooglePalm\n",
    "\n",
    "api_key = 'AIzaSyDlXYnP2xNNa0DNa7dPN89u2L4IuAchEg4'\n",
    "\n",
    "llm = GooglePalm(google_api_key=api_key, temperature=0.2)\n",
    "from langchain.utilities import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "\n",
    "db_user = \"root\"\n",
    "db_password = \"root\"\n",
    "db_host = \"localhost\"\n",
    "db_name = \"atliq_tshirts\"\n",
    "\n",
    "db = SQLDatabase.from_uri(f\"mysql+pymysql://{db_user}:{db_password}@{db_host}/{db_name}\",sample_rows_in_table_info=3)\n",
    "\n",
    "print(db.table_info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)\n",
    "qns1 = db_chain(\"How many t-shirts do we have left for nike in extra small size and white color?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "It made a mistake here. The price is actually the price per unit but in real life database columns will not have perfect names. We need to tell it somehow that price is price per unit and the actual query should be,\n",
    "\n",
    "SELECT SUM(price*stock_quantity) FROM t_shirts WHERE size = 'S'\n",
    "\n",
    "qns2 = db_chain.run(\"SELECT SUM(price*stock_quantity) FROM t_shirts WHERE size = 'S'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qns2 = db_chain.run(\"How much is the price of the inventory for all small size t-shirts?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qns3 = db_chain.run(\"If we have to sell all the Levi’s T-shirts today with discounts applied. How much revenue our store will generate (post discounts)?\")\n",
    "\n",
    "#Above, it returned a wrong query which generated an error during query execution.\n",
    "#  It thinks discount table would have start and end date which is normally true but\n",
    "#  in our table there is no start or end date column. One thing we can do here is run the query directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few shot learning : LLM hardcoding / expection handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Creating Semantic Similarity Based example selector\n",
    "\n",
    "    create embedding on the few_shots\n",
    "    Store the embeddings in Chroma DB\n",
    "    Retrieve the the top most Semantically close example from the vector store\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shots = [\n",
    "    {'Question' : \"How many t-shirts do we have left for Nike in XS size and white color?\",\n",
    "     'SQLQuery' : \"SELECT sum(stock_quantity) FROM t_shirts WHERE brand = 'Nike' AND color = 'White' AND size = 'XS'\",\n",
    "     'SQLResult': \"Result of the SQL query\",\n",
    "     'Answer' : qns1},\n",
    "    {'Question': \"How much is the total price of the inventory for all S-size t-shirts?\",\n",
    "     'SQLQuery':\"SELECT SUM(price*stock_quantity) FROM t_shirts WHERE size = 'S'\",\n",
    "     'SQLResult': \"Result of the SQL query\",\n",
    "     'Answer': qns2},\n",
    "    {'Question': \"If we have to sell all the Levi’s T-shirts today with discounts applied. How much revenue  our store will generate (post discounts)?\" ,\n",
    "     'SQLQuery' : \"\"\"SELECT sum(a.total_amount * ((100-COALESCE(discounts.pct_discount,0))/100)) as total_revenue from\n",
    "(select sum(price*stock_quantity) as total_amount, t_shirt_id from t_shirts where brand = 'Levi'\n",
    "group by t_shirt_id) a left join discounts on a.t_shirt_id = discounts.t_shirt_id\n",
    " \"\"\",\n",
    "     'SQLResult': \"Result of the SQL query\",\n",
    "     'Answer': qns3} ,\n",
    "     {'Question' : \"If we have to sell all the Levi’s T-shirts today. How much revenue our store will generate without discount?\" ,\n",
    "      'SQLQuery': \"SELECT SUM(price * stock_quantity) FROM t_shirts WHERE brand = 'Levi'\",\n",
    "      'SQLResult': \"Result of the SQL query\",\n",
    "      'Answer' : qns4},\n",
    "    {'Question': \"How many white color Levi's shirt I have?\",\n",
    "     'SQLQuery' : \"SELECT sum(stock_quantity) FROM t_shirts WHERE brand = 'Levi' AND color = 'White'\",\n",
    "     'SQLResult': \"Result of the SQL query\",\n",
    "     'Answer' : qns5\n",
    "     }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import SemanticSimilarityExampleSelector\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "to_vectorize = [\" \".join(example.values()) for example in few_shots]\n",
    "\n",
    "to_vectorize\n",
    "\n",
    "vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=few_shots)\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector(\n",
    "    vectorstore=vectorstore,\n",
    "    k=2,\n",
    ")\n",
    "\n",
    "example_selector.select_examples({\"Question\": \"How many Adidas T shirts I have left in my store?\"})\n",
    "\n",
    "### my sql based instruction prompt\n",
    "mysql_prompt = \"\"\"You are a MySQL expert. Given an input question, first create a syntactically correct MySQL query to run, then look at the results of the query and return the answer to the input question.\n",
    "Unless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per MySQL. You can order the results to return the most informative data in the database.\n",
    "Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in backticks (`) to denote them as delimited identifiers.\n",
    "Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n",
    "Pay attention to use CURDATE() function to get the current date, if the question involves \"today\".\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: Question here\n",
    "SQLQuery: Query to run with no pre-amble\n",
    "SQLResult: Result of the SQLQuery\n",
    "Answer: Final answer here\n",
    "\n",
    "No pre-amble.\n",
    "\"\"\"\n",
    "\n",
    "from langchain.prompts import FewShotPromptTemplate\n",
    "from langchain.chains.sql_database.prompt import PROMPT_SUFFIX, _mysql_prompt\n",
    "\n",
    "print(PROMPT_SUFFIX)\n",
    "\n",
    "# Setting up PromptTemplete using input variables\n",
    "\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"Question\", \"SQLQuery\", \"SQLResult\",\"Answer\",],\n",
    "    template=\"\\nQuestion: {Question}\\nSQLQuery: {SQLQuery}\\nSQLResult: {SQLResult}\\nAnswer: {Answer}\",\n",
    ")\n",
    "\n",
    "print(_mysql_prompt)\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=mysql_prompt,\n",
    "    suffix=PROMPT_SUFFIX,\n",
    "    input_variables=[\"input\", \"table_info\", \"top_k\"], #These variables are used in the prefix and suffix\n",
    ")\n",
    "\n",
    "new_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True, prompt=few_shot_prompt)\n",
    "\n",
    "new_chain(\"How many white color Levi's shirt I have?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chain(\"How much is the price of the inventory for all small size t-shirts?\")\n",
    "\n",
    "new_chain(\"How much is the price of all white color levi t shirts?\")\n",
    "\n",
    "new_chain(\"If we have to sell all the Nike’s T-shirts today with discounts applied. How much revenue  our store will generate (post discounts)?\")\n",
    "\n",
    "new_chain(\"If we have to sell all the Van Heuson T-shirts today with discounts applied. How much revenue  our store will generate (post discounts)?\")\n",
    "\n",
    "new_chain.run('How much revenue  our store will generate by selling all Van Heuson TShirts without discount?')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# openapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from secret_key import openai_key\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "openai.api_key = openai_key\n",
    "\n",
    "def extract_financial_data(text):\n",
    "    prompt = get_prompt_financial() + text\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\",\"content\": prompt}]\n",
    "    )\n",
    "    content = response.choices[0]['message']['content']\n",
    "\n",
    "    try:\n",
    "        data = json.loads(content)\n",
    "        return pd.DataFrame(data.items(), columns=[\"Measure\", \"Value\"])\n",
    "\n",
    "    except (json.JSONDecodeError, IndexError):\n",
    "        pass\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"Measure\": [\"Company Name\", \"Stock Symbol\", \"Revenue\", \"Net Income\", \"EPS\"],\n",
    "        \"Value\": [\"\", \"\", \"\", \"\", \"\"]\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "def get_prompt_financial():\n",
    "    return '''Please retrieve company name, revenue, net income and earnings per share (a.k.a. EPS)\n",
    "    from the following news article. If you can't find the information from this article \n",
    "    then return \"\". Do not make things up.    \n",
    "    Then retrieve a stock symbol corresponding to that company. For this you can use\n",
    "    your general knowledge (it doesn't have to be from this article). Always return your\n",
    "    response as a valid JSON string. The format of that string should be this, \n",
    "    {\n",
    "        \"Company Name\": \"Walmart\",\n",
    "        \"Stock Symbol\": \"WMT\",\n",
    "        \"Revenue\": \"12.34 million\",\n",
    "        \"Net Income\": \"34.78 million\",\n",
    "        \"EPS\": \"2.1 $\"\n",
    "    }\n",
    "    News Article:\n",
    "    ============\n",
    "\n",
    "    '''\n",
    "\n",
    "# Press the green button in the gutter to run the script.\n",
    "if __name__ == '__main__':\n",
    "    text = '''\n",
    "    Tesla's Earning news in text format: Tesla's earning this quarter blew all the estimates. They reported 4.5 billion $ profit against a revenue of 30 billion $. Their earnings per share was 2.3 $\n",
    "    '''\n",
    "    df = extract_financial_data(text)\n",
    "\n",
    "    print(df.to_string())\n",
    "\n",
    "    import streamlit as st\n",
    "import pandas as pd\n",
    "import openai_helper\n",
    "\n",
    "col1, col2 = st.columns([3,2])\n",
    "\n",
    "financial_data_df = pd.DataFrame({\n",
    "        \"Measure\": [\"Company Name\", \"Stock Symbol\", \"Revenue\", \"Net Income\", \"EPS\"],\n",
    "        \"Value\": [\"\", \"\", \"\", \"\", \"\"]\n",
    "    })\n",
    "\n",
    "with col1:\n",
    "    st.title(\"Data Extraction Tool\")\n",
    "    news_article = st.text_area(\"Paste your financial news article here\", height=300)\n",
    "    if st.button(\"Extract\"):\n",
    "        financial_data_df = openai_helper.extract_financial_data(news_article)\n",
    "\n",
    "with col2:\n",
    "    st.markdown(\"<br/>\" * 5, unsafe_allow_html=True)  # Creates 5 lines of vertical space\n",
    "    st.dataframe(\n",
    "        financial_data_df,\n",
    "        column_config={\n",
    "            \"Measure\": st.column_config.Column(width=150),\n",
    "            \"Value\": st.column_config.Column(width=150)\n",
    "        },\n",
    "        hide_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "def get_db_cursor():\n",
    "    # Open database connection\n",
    "    db = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"root\",\n",
    "        database=\"atliq_college_db\"\n",
    "    )\n",
    "\n",
    "    # Create a new cursor instance\n",
    "    cursor = db.cursor()\n",
    "\n",
    "    return db, cursor\n",
    "\n",
    "\n",
    "def close_db_connection(db, cursor):\n",
    "    # disconnect from server\n",
    "    cursor.close()\n",
    "    db.close()\n",
    "\n",
    "\n",
    "def get_marks(params):\n",
    "    db, cursor = get_db_cursor()\n",
    "    cursor.callproc('get_marks', [params.get('student_name', ''), params.get('semester', ''), params.get('operation', '')])\n",
    "    result = None\n",
    "    for res in cursor.stored_results():\n",
    "        result = float(res.fetchone()[0])  # Fetch the first column of the first row\n",
    "    close_db_connection(db, cursor)\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_fees(params):\n",
    "    db, cursor = get_db_cursor()\n",
    "    cursor.callproc('get_fees', [params.get('student_name', ''), params.get('semester', ''), params.get('fees_type', '')])\n",
    "    result = None\n",
    "    for res in cursor.stored_results():\n",
    "        result = float(res.fetchone()[0])  # Fetch the first column of the first row\n",
    "    close_db_connection(db, cursor)\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(get_marks({\n",
    "        'semester': 4,\n",
    "        'operation': 'avg'\n",
    "    }))\n",
    "\n",
    "    print(get_fees({\n",
    "        'student_name': \"Peter Pandey\",\n",
    "        'semester': 1,\n",
    "        'fees_type': 'paid'\n",
    "    }))\n",
    "\n",
    "    import openai\n",
    "import json\n",
    "from secret_key import openai_api_key\n",
    "import db_helper\n",
    "\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "\n",
    "def get_answer(question):\n",
    "    messages = [{'role': 'user', 'content': question}]\n",
    "    functions = [\n",
    "        {\n",
    "            \"name\": \"get_marks\",\n",
    "            \"description\": \"\"\"Get the GPA for a college student or aggregate GPA (such as average, min, max) \n",
    "            for a given semester. If function returns -1 then it means we could not find the record in a database for given input. \n",
    "            \"\"\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"student_name\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"First and last Name of the student. e.g John Smith\",\n",
    "                    },\n",
    "                    \"semester\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"A number between 1 to 4 indicating the semester of a student\",\n",
    "                    },\n",
    "                    \"operation\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"\"\"If student is blank that means aggregate number such as max, min or average is being\n",
    "                            requested for an entire semester. semester must be passed in this case. If student field is blank and say \n",
    "                            they are passing 1 as a value in semester. Then operation parameter will tell if they need a maximum, minimum\n",
    "                            or an average GPA of all students in semester 1.\n",
    "                            \"\"\",\n",
    "                        \"enum\": [\"max\", \"min\", \"avg\"]\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"semester\"],\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"get_fees\",\n",
    "            \"description\": \"\"\"Get the fees for an individual student or total fees for an entire \n",
    "            semester. If function returns -1 then it means we could not find the record in a database for given input.\n",
    "            \"\"\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"student_name\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"First and last Name of the student. e.g John Smith\",\n",
    "                    },\n",
    "                    \"semester\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"A number between 1 to 4 indicating the semester of a student\",\n",
    "                    },\n",
    "                    \"fees_type\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"fee type such as 'paid', 'pending' or 'total'\",\n",
    "                        \"enum\": [\"paid\", \"pending\", \"total\"]\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"semester\"],\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-0613\",\n",
    "        messages=messages,\n",
    "        functions=functions,\n",
    "        function_call=\"auto\",  # auto is default, but we'll be explicit\n",
    "    )\n",
    "    response_message = response[\"choices\"][0][\"message\"]\n",
    "\n",
    "    if response_message.get(\"function_call\"):\n",
    "        available_functions = {\n",
    "            \"get_marks\": db_helper.get_marks,\n",
    "            \"get_fees\": db_helper.get_fees\n",
    "        }\n",
    "        function_name = response_message[\"function_call\"][\"name\"]\n",
    "        fuction_to_call = available_functions[function_name]\n",
    "        function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n",
    "        function_response = fuction_to_call(function_args)\n",
    "\n",
    "        messages.append(response_message)\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"function\",\n",
    "                \"name\": function_name,\n",
    "                \"content\": str(function_response),\n",
    "            }\n",
    "        )\n",
    "        second_response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-0613\",\n",
    "            messages=messages,\n",
    "        )  # get a new response from GPT where it can see the function response\n",
    "        return second_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        return response_message[\"content\"]\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # print(get_answer(\"What was Peter Pandey's GPA in semester 1?\"))\n",
    "    # print(get_answer(\"average gpa in third semester?\"))\n",
    "    # print(get_answer(\"how much is peter pandey's pending fees in the first semester?\"))\n",
    "    print(get_answer(\"how much was peter pandey's due fees in the first semester?\"))\n",
    "    # print(get_answer(\"what is the purpose of a balance sheet?\"))\n",
    "\n",
    "    import streamlit as st\n",
    "from openai_helper import get_answer\n",
    "\n",
    "st.title(\"AtliQ College: Q&A System\")\n",
    "\n",
    "question = st.text_input(\"Question:\")\n",
    "\n",
    "if question:\n",
    "    answer = get_answer(question)\n",
    "    st.text(\"Answer:\")\n",
    "    st.write(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
